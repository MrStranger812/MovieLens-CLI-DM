{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ MovieLens\n",
    "\n",
    "## Ù…Ù‚Ø¯Ù…Ù‡\n",
    "Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ ØªÛŒÙ… ØªÙˆØ³Ø¹Ù‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "\n",
    "### Ù…Ø­ØªÙˆÛŒØ§Øª:\n",
    "1. **Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§**: Ù†Ø­ÙˆÙ‡ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡\n",
    "2. **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†**: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² ÙÛŒÙ„Ù…â€ŒÙ‡Ø§\n",
    "3. **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ**: ØªØ´Ø®ÛŒØµ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨\n",
    "4. **Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ**: Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§\n",
    "5. **Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ù†Ø¬Ù…Ù†ÛŒ**: Ú©Ø´Ù Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø±ÛŒ\n",
    "6. **Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ…**: Ø³ÛŒØ³ØªÙ… ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø±\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯ âœ…\n"
     ]
    }
   ],
   "source": [
    "# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "import gzip\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ matplotlib\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ¨Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']\n",
    "\n",
    "print(\"Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯ âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\n",
      "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§ØªØ±ÛŒØ³: [Errno 2] No such file or directory: './data/processed/user_item_matrix.npz'\n",
      "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ML datasets: [Errno 2] No such file or directory: './data/processed/ml_ready_datasets.pkl.gz'\n",
      "âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ features: [Errno 2] No such file or directory: './data/processed/hyper_features.pkl.gz'\n",
      "\n",
      "âœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ…\n",
    "    try:\n",
    "        data['user_item_matrix'] = sparse.load_npz('./data/processed/user_item_matrix.npz')\n",
    "        print(\"âœ… Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø§ØªØ±ÛŒØ³: {e}\")\n",
    "    \n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ML\n",
    "    try:\n",
    "        with gzip.open('./data/processed/ml_ready_datasets.pkl.gz', 'rb') as f:\n",
    "            data['ml_datasets'] = pickle.load(f)\n",
    "        print(\"âœ… Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ML datasets: {e}\")\n",
    "    \n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
    "    try:\n",
    "        with gzip.open('./data/processed/hyper_features.pkl.gz', 'rb') as f:\n",
    "            data['features'] = pickle.load(f)\n",
    "        print(\"âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ features: {e}\")\n",
    "    \n",
    "    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ Ø§Ø² parquet\n",
    "    try:\n",
    "        data['user_features'] = pd.read_parquet('./data/processed/user_features.parquet')\n",
    "        print(\"âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        data['movie_features'] = pd.read_parquet('data/processed/movie_features.parquet')\n",
    "        print(\"âœ… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "print(\"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\")\n",
    "data = load_all_data()\n",
    "print(\"\\nâœ… Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ\n",
    "print(\"ğŸ“Š Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡:\\n\")\n",
    "\n",
    "if 'ml_datasets' in data:\n",
    "    print(\"ğŸ¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†:\")\n",
    "    for task_name, task_data in data['ml_datasets'].items():\n",
    "        if task_data is not None:\n",
    "            print(f\"\\n  ğŸ“Œ {task_name}:\")\n",
    "            if isinstance(task_data, dict):\n",
    "                for key, value in task_data.items():\n",
    "                    if hasattr(value, 'shape'):\n",
    "                        print(f\"    - {key}: Ø´Ú©Ù„ {value.shape}\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"    - {key}: {len(value)} Ø¢ÛŒØªÙ…\")\n",
    "                    elif isinstance(value, dict):\n",
    "                        print(f\"    - {key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"    - {key}: {type(value).__name__}\")\n",
    "\n",
    "if 'user_item_matrix' in data:\n",
    "    matrix = data['user_item_matrix']\n",
    "    print(f\"\\nğŸ“Š Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ…:\")\n",
    "    print(f\"  - Ø§Ø¨Ø¹Ø§Ø¯: {matrix.shape[0]:,} Ú©Ø§Ø±Ø¨Ø± Ã— {matrix.shape[1]:,} ÙÛŒÙ„Ù…\")\n",
    "    print(f\"  - ØªØ¹Ø¯Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª: {matrix.nnz:,}\")\n",
    "    print(f\"  - Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ: {(1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ† - Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ ğŸ“ˆ\n",
    "\n",
    "Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø±ÙˆÛŒ **Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² (rating)** Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'regression' in data['ml_datasets']:\n",
    "    reg_data = data['ml_datasets']['regression']\n",
    "    \n",
    "    if reg_data and 'X' in reg_data:\n",
    "        X_reg = reg_data['X']\n",
    "        y_reg = reg_data['y']\n",
    "        \n",
    "        print(\"ğŸ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØªØ§Ø³Øª Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†:\\n\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {X_reg.shape[0]:,}\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_reg.shape[1]}\")\n",
    "        print(f\"\\nØ¨Ø§Ø²Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª: {y_reg.min():.1f} ØªØ§ {y_reg.max():.1f}\")\n",
    "        print(f\"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª: {y_reg.mean():.2f}\")\n",
    "        print(f\"Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {y_reg.std():.2f}\")\n",
    "        \n",
    "        # Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # ØªÙˆØ²ÛŒØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª\n",
    "        axes[0].hist(y_reg, bins=20, color=colors[0], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title('ØªÙˆØ²ÛŒØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù‡Ø¯Ù', fontsize=14, pad=10)\n",
    "        axes[0].set_xlabel('Ø§Ù…ØªÛŒØ§Ø²')\n",
    "        axes[0].set_ylabel('ØªØ¹Ø¯Ø§Ø¯')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ\n",
    "        axes[1].boxplot(y_reg, vert=True, patch_artist=True,\n",
    "                       boxprops=dict(facecolor=colors[1], alpha=0.7),\n",
    "                       medianprops=dict(color='red', linewidth=2))\n",
    "        axes[1].set_title('Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª', fontsize=14, pad=10)\n",
    "        axes[1].set_ylabel('Ø§Ù…ØªÛŒØ§Ø²')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ØªÙˆØ²ÛŒØ¹ ØªØ¬Ù…Ø¹ÛŒ\n",
    "        sorted_ratings = np.sort(y_reg)\n",
    "        p = np.arange(len(sorted_ratings)) / len(sorted_ratings)\n",
    "        axes[2].plot(sorted_ratings, p, color=colors[2], linewidth=2)\n",
    "        axes[2].set_title('ØªÙˆØ²ÛŒØ¹ ØªØ¬Ù…Ø¹ÛŒ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª', fontsize=14, pad=10)\n",
    "        axes[2].set_xlabel('Ø§Ù…ØªÛŒØ§Ø²')\n",
    "        axes[2].set_ylabel('Ø§Ø­ØªÙ…Ø§Ù„ ØªØ¬Ù…Ø¹ÛŒ')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Ù†Ù…Ø§ÛŒØ´ Ù†Ø§Ù… ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "        if 'feature_names' in reg_data:\n",
    "            print(\"\\nğŸ“‹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ (Ù†Ù…ÙˆÙ†Ù‡):\")\n",
    "            features = reg_data['feature_names']\n",
    "            for i in range(min(10, len(features))):\n",
    "                print(f\"  {i+1}. {features[i]}\")\n",
    "            if len(features) > 10:\n",
    "                print(f\"  ... Ùˆ {len(features)-10} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯ÛŒÚ¯Ø±\")\n",
    "        \n",
    "        # Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
    "        print(\"\\nğŸ’¡ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡:\")\n",
    "        print(\"\"\"```python\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "X_train = reg_data['X']\n",
    "y_train = reg_data['y']\n",
    "\n",
    "# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ù…Ø¯Ù„ Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'MSE: {mse:.3f}, RÂ²: {r2:.3f}')\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ - ØªØ´Ø®ÛŒØµ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ ğŸ¯\n",
    "\n",
    "Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø±ÙˆÛŒ **Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯ÙˆØ¯ÙˆÛŒÛŒ** (ÙÛŒÙ„Ù… Ø®ÙˆØ¨/Ø¨Ø¯) Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'classification' in data['ml_datasets']:\n",
    "    clf_data = data['ml_datasets']['classification']\n",
    "    \n",
    "    if clf_data and 'X' in clf_data:\n",
    "        X_clf = clf_data['X']\n",
    "        y_clf = clf_data['y']\n",
    "        \n",
    "        print(\"ğŸ¯ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯ÛŒØªØ§Ø³Øª Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:\\n\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {X_clf.shape[0]:,}\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_clf.shape[1]}\")\n",
    "        \n",
    "        # ØªÙˆØ§Ø²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
    "        if 'class_balance' in clf_data:\n",
    "            balance = clf_data['class_balance']\n",
    "            print(f\"\\nØªÙˆØ§Ø²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§:\")\n",
    "            print(f\"  - ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ (Ø§Ù…ØªÛŒØ§Ø² â‰¥ 4): {balance.get('positive', balance.get(1, 0)):.1%}\")\n",
    "            print(f\"  - ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ (Ø§Ù…ØªÛŒØ§Ø² < 4): {balance.get('negative', balance.get(0, 0)):.1%}\")\n",
    "        else:\n",
    "            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø³ØªÛŒ ØªÙˆØ§Ø²Ù†\n",
    "            unique, counts = np.unique(y_clf, return_counts=True)\n",
    "            balance_dict = dict(zip(unique, counts))\n",
    "            total = sum(counts)\n",
    "            \n",
    "        # Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ ØªÙˆØ§Ø²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
    "        if 'class_balance' in clf_data:\n",
    "            labels = ['Ù…Ø­Ø¨ÙˆØ¨ (â‰¥4)', 'Ù…Ø¹Ù…ÙˆÙ„ÛŒ (<4)']\n",
    "            sizes = [balance.get('positive', balance.get(1, 0)), \n",
    "                    balance.get('negative', balance.get(0, 0))]\n",
    "            axes[0].pie(sizes, labels=labels, autopct='%1.1f%%', \n",
    "                       colors=[colors[2], colors[3]], startangle=90)\n",
    "        else:\n",
    "            labels = [f'Ú©Ù„Ø§Ø³ {c}' for c in unique]\n",
    "            axes[0].pie(counts, labels=labels, autopct='%1.1f%%', \n",
    "                       colors=colors[:len(unique)], startangle=90)\n",
    "        axes[0].set_title('ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§', fontsize=14, pad=20)\n",
    "        \n",
    "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ\n",
    "        unique_classes, class_counts = np.unique(y_clf, return_counts=True)\n",
    "        axes[1].bar(unique_classes, class_counts, color=[colors[4], colors[5]])\n",
    "        axes[1].set_title('ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³', fontsize=14, pad=10)\n",
    "        axes[1].set_xlabel('Ú©Ù„Ø§Ø³')\n",
    "        axes[1].set_ylabel('ØªØ¹Ø¯Ø§Ø¯')\n",
    "        axes[1].set_xticks(unique_classes)\n",
    "        axes[1].set_xticklabels(['Ù…Ø¹Ù…ÙˆÙ„ÛŒ', 'Ù…Ø­Ø¨ÙˆØ¨'])\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯\n",
    "        print(\"\\nğŸ’¡ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡:\")\n",
    "        print(\"\"\"```python\n",
    "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "X = clf_data['X']\n",
    "y = clf_data['y']\n",
    "\n",
    "# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, \n",
    "                          target_names=['Ù…Ø¹Ù…ÙˆÙ„ÛŒ', 'Ù…Ø­Ø¨ÙˆØ¨']))\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ - Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ ğŸ¨\n",
    "\n",
    "Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø±ÙˆÛŒ **Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ** Ùˆ **ØªØ­Ù„ÛŒÙ„ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§** Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "if 'ml_datasets' in data and 'clustering_users' in data['ml_datasets']:\n",
    "    user_cluster_data = data['ml_datasets']['clustering_users']\n",
    "    \n",
    "    if user_cluster_data and 'X' in user_cluster_data:\n",
    "        X_users = user_cluster_data['X']\n",
    "        \n",
    "        print(\"ğŸ‘¥ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†:\\n\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {X_users.shape[0]:,}\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_users.shape[1]}\")\n",
    "        \n",
    "        if 'feature_names' in user_cluster_data:\n",
    "            print(\"\\nÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†:\")\n",
    "            for i, feat in enumerate(user_cluster_data['feature_names'][:5]):\n",
    "                print(f\"  {i+1}. {feat}\")\n",
    "        \n",
    "        # ØªØ¬Ø³Ù… Ø¨Ø§ t-SNE\n",
    "        if X_users.shape[0] > 1000:\n",
    "            sample_indices = np.random.choice(X_users.shape[0], 1000, replace=False)\n",
    "            X_sample = X_users[sample_indices]\n",
    "        else:\n",
    "            X_sample = X_users\n",
    "        \n",
    "        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_sample)\n",
    "        \n",
    "        # t-SNE\n",
    "        print(\"\\nğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        X_tsne = tsne.fit_transform(X_scaled)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                            c=range(len(X_tsne)), cmap='viridis', \n",
    "                            alpha=0.6, s=50)\n",
    "        plt.title('Ù†Ù…Ø§ÛŒØ´ Ø¯ÙˆØ¨Ø¹Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø§ t-SNE', fontsize=16, pad=20)\n",
    "        plt.xlabel('Ø¨Ø¹Ø¯ Ø§ÙˆÙ„')\n",
    "        plt.ylabel('Ø¨Ø¹Ø¯ Ø¯ÙˆÙ…')\n",
    "        plt.colorbar(scatter, label='Ø´Ø§Ø®Øµ Ú©Ø§Ø±Ø¨Ø±')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§\n",
    "if 'ml_datasets' in data and 'clustering_movies' in data['ml_datasets']:\n",
    "    movie_cluster_data = data['ml_datasets']['clustering_movies']\n",
    "    \n",
    "    if movie_cluster_data and 'X' in movie_cluster_data:\n",
    "        X_movies = movie_cluster_data['X']\n",
    "        \n",
    "        print(\"\\nğŸ¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§:\\n\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§: {X_movies.shape[0]:,}\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {X_movies.shape[1]}\")\n",
    "        \n",
    "        # Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯\n",
    "        print(\"\\nğŸ’¡ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:\")\n",
    "        print(\"\"\"```python\n",
    "# Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(user_cluster_data['X'])\n",
    "\n",
    "# ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Ø§Ù†ØªØ®Ø§Ø¨ k=5 Ùˆ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "user_clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
    "for i in range(5):\n",
    "    cluster_users = np.where(user_clusters == i)[0]\n",
    "    print(f'Ø®ÙˆØ´Ù‡ {i}: {len(cluster_users)} Ú©Ø§Ø±Ø¨Ø±')\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ù†Ø¬Ù…Ù†ÛŒ - Ú©Ø´Ù Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø±ÛŒ ğŸ”\n",
    "\n",
    "Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø±ÙˆÛŒ **Ú©Ø´Ù Ø§Ù„Ú¯ÙˆÙ‡Ø§** Ùˆ **Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ù†Ø¬Ù…Ù†ÛŒ** Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'association_rules' in data['ml_datasets']:\n",
    "    assoc_data = data['ml_datasets']['association_rules']\n",
    "    \n",
    "    if assoc_data and 'transactions' in assoc_data:\n",
    "        transactions = assoc_data['transactions']\n",
    "        \n",
    "        print(\"ğŸ” Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ù†Ø¬Ù…Ù†ÛŒ:\\n\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§ (Ú©Ø§Ø±Ø¨Ø±Ø§Ù†): {len(transactions):,}\")\n",
    "        print(f\"Ø­Ø¯Ø§Ù‚Ù„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ (min_support): {assoc_data.get('min_support', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\")\n",
    "        print(f\"Ø­Ø¯Ø§Ù‚Ù„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† (min_confidence): {assoc_data.get('min_confidence', 'Ù†Ø§Ù…Ø´Ø®Øµ')}\")\n",
    "        \n",
    "        # Ø¢Ù…Ø§Ø± ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§\n",
    "        trans_lengths = [len(t) for t in transactions if t]\n",
    "        if trans_lengths:\n",
    "            print(f\"\\nØ¢Ù…Ø§Ø± ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§:\")\n",
    "            print(f\"  - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± ØªØ±Ø§Ú©Ù†Ø´: {np.mean(trans_lengths):.1f}\")\n",
    "            print(f\"  - Ø­Ø¯Ø§Ù‚Ù„: {np.min(trans_lengths)}\")\n",
    "            print(f\"  - Ø­Ø¯Ø§Ú©Ø«Ø±: {np.max(trans_lengths)}\")\n",
    "            \n",
    "            # Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(trans_lengths, bins=50, color=colors[6], alpha=0.7, edgecolor='black')\n",
    "            plt.title('ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ Ù‡Ø± Ú©Ø§Ø±Ø¨Ø±', fontsize=14, pad=10)\n",
    "            plt.xlabel('ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§')\n",
    "            plt.ylabel('ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        \n",
    "        # Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯\n",
    "        print(\"\\nğŸ’¡ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ù‚ÙˆØ§Ù†ÛŒÙ†:\")\n",
    "        print(\"\"\"```python\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# ØªØ¨Ø¯ÛŒÙ„ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Ú©Ø´Ù Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ú©Ø±Ø±\n",
    "frequent_items = apriori(df_transactions, \n",
    "                        min_support=0.01, \n",
    "                        use_colnames=True)\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚ÙˆØ§Ù†ÛŒÙ†\n",
    "rules = association_rules(frequent_items, \n",
    "                         metric=\"confidence\", \n",
    "                         min_threshold=0.5)\n",
    "\n",
    "# Ù†Ù…Ø§ÛŒØ´ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø¨Ø±ØªØ±\n",
    "print(\"Ù‚ÙˆØ§Ù†ÛŒÙ† Ø¨Ø§ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ø·Ù…ÛŒÙ†Ø§Ù†:\")\n",
    "print(rules.nlargest(10, 'confidence')[['antecedents', 'consequents', \n",
    "                                        'support', 'confidence', 'lift']])\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ… - Ø³ÛŒØ³ØªÙ… ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø± ğŸ¬\n",
    "\n",
    "Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†ÛŒ Ú©Ù‡ Ø±ÙˆÛŒ **Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø±** Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'user_item_matrix' in data:\n",
    "    matrix = data['user_item_matrix']\n",
    "    \n",
    "    print(\"ğŸ¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ…:\\n\")\n",
    "    print(f\"Ø§Ø¨Ø¹Ø§Ø¯: {matrix.shape[0]:,} Ú©Ø§Ø±Ø¨Ø± Ã— {matrix.shape[1]:,} ÙÛŒÙ„Ù…\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø«Ø¨Øª Ø´Ø¯Ù‡: {matrix.nnz:,}\")\n",
    "    print(f\"Ø¯Ø±ØµØ¯ Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ: {(1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    # ØªÙˆØ²ÛŒØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª\n",
    "    ratings = matrix.data\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø§Ù…ØªÛŒØ§Ø²Ø§Øª\n",
    "    axes[0].hist(ratings, bins=np.arange(0.5, 6, 0.5), \n",
    "                color=colors[0], alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('ØªÙˆØ²ÛŒØ¹ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ø¯Ø± Ù…Ø§ØªØ±ÛŒØ³', fontsize=14, pad=10)\n",
    "    axes[0].set_xlabel('Ø§Ù…ØªÛŒØ§Ø²')\n",
    "    axes[0].set_ylabel('ØªØ¹Ø¯Ø§Ø¯')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ø¢Ù…Ø§Ø± Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "    user_counts = np.array((matrix != 0).sum(axis=1)).flatten()\n",
    "    axes[1].hist(user_counts[user_counts > 0], bins=50, \n",
    "                color=colors[1], alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ù‡Ø± Ú©Ø§Ø±Ø¨Ø±', fontsize=14, pad=10)\n",
    "    axes[1].set_xlabel('ØªØ¹Ø¯Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²')\n",
    "    axes[1].set_ylabel('ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯\n",
    "    print(\"\\nğŸ’¡ Ù†Ù…ÙˆÙ†Ù‡ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø±:\")\n",
    "    print(\"\"\"```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "user_similarity = cosine_similarity(matrix, dense_output=False)\n",
    "\n",
    "# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± i Ùˆ ÙÛŒÙ„Ù… j\n",
    "def predict_rating(user_id, movie_id, k=50):\n",
    "    # ÛŒØ§ÙØªÙ† k Ú©Ø§Ø±Ø¨Ø± Ù…Ø´Ø§Ø¨Ù‡\n",
    "    user_sim = user_similarity[user_id].toarray().flatten()\n",
    "    similar_users = np.argsort(user_sim)[-k-1:-1][::-1]\n",
    "    \n",
    "    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÙˆØ²Ù†â€ŒØ¯Ø§Ø±\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for similar_user in similar_users:\n",
    "        if matrix[similar_user, movie_id] > 0:\n",
    "            numerator += user_sim[similar_user] * matrix[similar_user, movie_id]\n",
    "            denominator += user_sim[similar_user]\n",
    "    \n",
    "    if denominator > 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return matrix[user_id].mean()  # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²Ø§Øª Ú©Ø§Ø±Ø¨Ø±\n",
    "\n",
    "# ØªÙˆØµÛŒÙ‡ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ø±Ø¨Ø±\n",
    "def recommend_movies(user_id, n_recommendations=10):\n",
    "    # ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù†Ø¯ÛŒØ¯Ù‡\n",
    "    user_ratings = matrix[user_id].toarray().flatten()\n",
    "    unseen_movies = np.where(user_ratings == 0)[0]\n",
    "    \n",
    "    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù†Ø¯ÛŒØ¯Ù‡\n",
    "    predictions = []\n",
    "    for movie_id in unseen_movies[:1000]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 1000 ÙÛŒÙ„Ù… Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª\n",
    "        pred = predict_rating(user_id, movie_id)\n",
    "        predictions.append((movie_id, pred))\n",
    "    \n",
    "    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:n_recommendations]\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ ğŸ“Š\n",
    "\n",
    "Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†\n",
    "if 'user_features' in data:\n",
    "    user_features = data['user_features']\n",
    "    print(\"ğŸ‘¥ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†:\\n\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {len(user_features):,}\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(user_features.columns)}\")\n",
    "    print(\"\\nÙ†Ù…ÙˆÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:\")\n",
    "    for col in user_features.columns[:10]:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ\n",
    "    print(\"\\nØ¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ (5 ÙˆÛŒÚ˜Ú¯ÛŒ Ø§ÙˆÙ„):\")\n",
    "    print(user_features.iloc[:, :5].describe().round(2))\n",
    "\n",
    "# ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§\n",
    "if 'movie_features' in data:\n",
    "    movie_features = data['movie_features']\n",
    "    print(\"\\n\\nğŸ¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§:\\n\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§: {len(movie_features):,}\")\n",
    "    print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(movie_features.columns)}\")\n",
    "    print(\"\\nÙ†Ù…ÙˆÙ†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:\")\n",
    "    for col in movie_features.columns[:10]:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ ğŸ“š\n",
    "\n",
    "### Ø®Ù„Ø§ØµÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯:\n",
    "\n",
    "1. **Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†**: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù…ØªÛŒØ§Ø² Ø¯Ù‚ÛŒÙ‚ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ (0.5 ØªØ§ 5)\n",
    "2. **Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ**: ØªØ´Ø®ÛŒØµ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¨ÙˆØ¨ (Ø§Ù…ØªÛŒØ§Ø² â‰¥ 4) Ø§Ø² Ù…Ø¹Ù…ÙˆÙ„ÛŒ\n",
    "3. **Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†**: Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙØªØ§Ø±\n",
    "4. **Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§**: Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
    "5. **Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ù†Ø¬Ù…Ù†ÛŒ**: Ú©Ø´Ù Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø´ØªØ±Ú© Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§\n",
    "6. **Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ø±Ø¨Ø±-Ø¢ÛŒØªÙ…**: Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø± Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ù…Ø´Ø§Ø±Ú©ØªÛŒ\n",
    "\n",
    "### Ù†Ú©Ø§Øª Ù…Ù‡Ù…:\n",
    "\n",
    "- Ù‡Ù…Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "- ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù‡Ø³ØªÙ†Ø¯\n",
    "- Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfittingØŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… train/test Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯\n",
    "- Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒØŒ Ø¨Ø§ ØªÛŒÙ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ú©Ù†ÛŒØ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØµØ¯ÙˆØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ğŸ’¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
    "def save_dataset_for_task(task_name, output_format='csv'):\n",
    "    \"\"\"Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª ÛŒÚ© ØªØ³Ú© Ø®Ø§Øµ\"\"\"\n",
    "    \n",
    "    if 'ml_datasets' not in data or task_name not in data['ml_datasets']:\n",
    "        print(f\"âŒ Ø¯ÛŒØªØ§Ø³Øª {task_name} ÛŒØ§ÙØª Ù†Ø´Ø¯!\")\n",
    "        return\n",
    "    \n",
    "    task_data = data['ml_datasets'][task_name]\n",
    "    if not task_data:\n",
    "        print(f\"âŒ Ø¯ÛŒØªØ§Ø³Øª {task_name} Ø®Ø§Ù„ÛŒ Ø§Ø³Øª!\")\n",
    "        return\n",
    "    \n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ø®Ø±ÙˆØ¬ÛŒ\n",
    "    import os\n",
    "    os.makedirs('exported_data', exist_ok=True)\n",
    "    \n",
    "    if 'X' in task_data and 'y' in task_data:\n",
    "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame\n",
    "        X_df = pd.DataFrame(task_data['X'])\n",
    "        if 'feature_names' in task_data:\n",
    "            X_df.columns = task_data['feature_names']\n",
    "        \n",
    "        y_df = pd.DataFrame(task_data['y'], columns=['target'])\n",
    "        \n",
    "        # ØªØ±Ú©ÛŒØ¨ X Ùˆ y\n",
    "        full_df = pd.concat([X_df, y_df], axis=1)\n",
    "        \n",
    "        # Ø°Ø®ÛŒØ±Ù‡\n",
    "        if output_format == 'csv':\n",
    "            full_df.to_csv(f'exported_data/{task_name}_dataset.csv', index=False)\n",
    "            print(f\"âœ… Ø¯ÛŒØªØ§Ø³Øª {task_name} Ø¯Ø± ÙØ§ÛŒÙ„ exported_data/{task_name}_dataset.csv Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯\")\n",
    "        elif output_format == 'parquet':\n",
    "            full_df.to_parquet(f'exported_data/{task_name}_dataset.parquet', index=False)\n",
    "            print(f\"âœ… Ø¯ÛŒØªØ§Ø³Øª {task_name} Ø¯Ø± ÙØ§ÛŒÙ„ exported_data/{task_name}_dataset.parquet Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯\")\n",
    "\n",
    "# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
    "print(\"ğŸ“¥ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:\\n\")\n",
    "print(\"save_dataset_for_task('regression', 'csv')    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø±Ú¯Ø±Ø³ÛŒÙˆÙ†\")\n",
    "print(\"save_dataset_for_task('classification', 'parquet')  # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ¯ Ù…ÙˆÙÙ‚ Ø¨Ø§Ø´ÛŒØ¯!\n",
    "\n",
    "Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© ØªÙˆØ³Ø· ØªÛŒÙ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± ÛŒØ§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒØŒ Ø¨Ø§ Ù…Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§Ø´ÛŒØ¯.\n",
    "\n",
    "**Ù†Ø³Ø®Ù‡**: 1.0  \n",
    "**ØªØ§Ø±ÛŒØ®**: Û±Û´Û°Û³/Û°Û¹/Û²Û°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

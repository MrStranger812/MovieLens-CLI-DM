{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# راهنمای جامع داده‌های پردازش شده MovieLens\n",
    "\n",
    "## مقدمه\n",
    "این نوت‌بوک برای آشنایی تیم توسعه با داده‌های پردازش شده و آماده‌سازی شده برای مدل‌های مختلف یادگیری ماشین طراحی شده است.\n",
    "\n",
    "### محتویات:\n",
    "1. **بارگذاری داده‌ها**: نحوه دسترسی به داده‌های پردازش شده\n",
    "2. **داده‌های رگرسیون**: پیش‌بینی امتیاز فیلم‌ها\n",
    "3. **داده‌های طبقه‌بندی**: تشخیص فیلم‌های محبوب\n",
    "4. **داده‌های خوشه‌بندی**: گروه‌بندی کاربران و فیلم‌ها\n",
    "5. **قوانین انجمنی**: کشف الگوهای رفتاری\n",
    "6. **ماتریس کاربر-آیتم**: سیستم توصیه‌گر\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کتابخانه‌ها با موفقیت بارگذاری شدند ✅\n"
     ]
    }
   ],
   "source": [
    "# وارد کردن کتابخانه‌های مورد نیاز\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pickle\n",
    "import gzip\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# تنظیمات فونت فارسی برای matplotlib\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# رنگ‌های زیبا برای نمودارها\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']\n",
    "\n",
    "print(\"کتابخانه‌ها با موفقیت بارگذاری شدند ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. بارگذاری داده‌های پردازش شده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 در حال بارگذاری داده‌ها...\n",
      "❌ خطا در بارگذاری ماتریس: [Errno 2] No such file or directory: './data/processed/user_item_matrix.npz'\n",
      "❌ خطا در بارگذاری ML datasets: [Errno 2] No such file or directory: './data/processed/ml_ready_datasets.pkl.gz'\n",
      "❌ خطا در بارگذاری features: [Errno 2] No such file or directory: './data/processed/hyper_features.pkl.gz'\n",
      "\n",
      "✅ بارگذاری کامل شد!\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"بارگذاری تمام داده‌های پردازش شده\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # بارگذاری ماتریس کاربر-آیتم\n",
    "    try:\n",
    "        data['user_item_matrix'] = sparse.load_npz('./data/processed/user_item_matrix.npz')\n",
    "        print(\"✅ ماتریس کاربر-آیتم بارگذاری شد\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ خطا در بارگذاری ماتریس: {e}\")\n",
    "    \n",
    "    # بارگذاری دیتاست‌های ML\n",
    "    try:\n",
    "        with gzip.open('./data/processed/ml_ready_datasets.pkl.gz', 'rb') as f:\n",
    "            data['ml_datasets'] = pickle.load(f)\n",
    "        print(\"✅ دیتاست‌های یادگیری ماشین بارگذاری شدند\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ خطا در بارگذاری ML datasets: {e}\")\n",
    "    \n",
    "    # بارگذاری ویژگی‌های پیشرفته\n",
    "    try:\n",
    "        with gzip.open('./data/processed/hyper_features.pkl.gz', 'rb') as f:\n",
    "            data['features'] = pickle.load(f)\n",
    "        print(\"✅ ویژگی‌های پیشرفته بارگذاری شدند\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ خطا در بارگذاری features: {e}\")\n",
    "    \n",
    "    # بارگذاری ویژگی‌های کاربران و فیلم‌ها از parquet\n",
    "    try:\n",
    "        data['user_features'] = pd.read_parquet('./data/processed/user_features.parquet')\n",
    "        print(\"✅ ویژگی‌های کاربران بارگذاری شد\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        data['movie_features'] = pd.read_parquet('data/processed/movie_features.parquet')\n",
    "        print(\"✅ ویژگی‌های فیلم‌ها بارگذاری شد\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return data\n",
    "\n",
    "# بارگذاری داده‌ها\n",
    "print(\"🔄 در حال بارگذاری داده‌ها...\")\n",
    "data = load_all_data()\n",
    "print(\"\\n✅ بارگذاری کامل شد!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. بررسی ساختار داده‌های بارگذاری شده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# نمایش اطلاعات کلی\n",
    "print(\"📊 ساختار داده‌های بارگذاری شده:\\n\")\n",
    "\n",
    "if 'ml_datasets' in data:\n",
    "    print(\"🎯 دیتاست‌های یادگیری ماشین:\")\n",
    "    for task_name, task_data in data['ml_datasets'].items():\n",
    "        if task_data is not None:\n",
    "            print(f\"\\n  📌 {task_name}:\")\n",
    "            if isinstance(task_data, dict):\n",
    "                for key, value in task_data.items():\n",
    "                    if hasattr(value, 'shape'):\n",
    "                        print(f\"    - {key}: شکل {value.shape}\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"    - {key}: {len(value)} آیتم\")\n",
    "                    elif isinstance(value, dict):\n",
    "                        print(f\"    - {key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"    - {key}: {type(value).__name__}\")\n",
    "\n",
    "if 'user_item_matrix' in data:\n",
    "    matrix = data['user_item_matrix']\n",
    "    print(f\"\\n📊 ماتریس کاربر-آیتم:\")\n",
    "    print(f\"  - ابعاد: {matrix.shape[0]:,} کاربر × {matrix.shape[1]:,} فیلم\")\n",
    "    print(f\"  - تعداد امتیازات: {matrix.nnz:,}\")\n",
    "    print(f\"  - پراکندگی: {(1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. داده‌های رگرسیون - پیش‌بینی امتیاز فیلم‌ها 📈\n",
    "\n",
    "این بخش برای توسعه‌دهندگانی که روی **پیش‌بینی امتیاز (rating)** کار می‌کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'regression' in data['ml_datasets']:\n",
    "    reg_data = data['ml_datasets']['regression']\n",
    "    \n",
    "    if reg_data and 'X' in reg_data:\n",
    "        X_reg = reg_data['X']\n",
    "        y_reg = reg_data['y']\n",
    "        \n",
    "        print(\"🎯 اطلاعات دیتاست رگرسیون:\\n\")\n",
    "        print(f\"تعداد نمونه‌ها: {X_reg.shape[0]:,}\")\n",
    "        print(f\"تعداد ویژگی‌ها: {X_reg.shape[1]}\")\n",
    "        print(f\"\\nبازه امتیازات: {y_reg.min():.1f} تا {y_reg.max():.1f}\")\n",
    "        print(f\"میانگین امتیازات: {y_reg.mean():.2f}\")\n",
    "        print(f\"انحراف معیار: {y_reg.std():.2f}\")\n",
    "        \n",
    "        # نمودار توزیع امتیازات\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # توزیع امتیازات\n",
    "        axes[0].hist(y_reg, bins=20, color=colors[0], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title('توزیع امتیازات هدف', fontsize=14, pad=10)\n",
    "        axes[0].set_xlabel('امتیاز')\n",
    "        axes[0].set_ylabel('تعداد')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # نمودار جعبه‌ای\n",
    "        axes[1].boxplot(y_reg, vert=True, patch_artist=True,\n",
    "                       boxprops=dict(facecolor=colors[1], alpha=0.7),\n",
    "                       medianprops=dict(color='red', linewidth=2))\n",
    "        axes[1].set_title('نمودار جعبه‌ای امتیازات', fontsize=14, pad=10)\n",
    "        axes[1].set_ylabel('امتیاز')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # توزیع تجمعی\n",
    "        sorted_ratings = np.sort(y_reg)\n",
    "        p = np.arange(len(sorted_ratings)) / len(sorted_ratings)\n",
    "        axes[2].plot(sorted_ratings, p, color=colors[2], linewidth=2)\n",
    "        axes[2].set_title('توزیع تجمعی امتیازات', fontsize=14, pad=10)\n",
    "        axes[2].set_xlabel('امتیاز')\n",
    "        axes[2].set_ylabel('احتمال تجمعی')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # نمایش نام ویژگی‌ها\n",
    "        if 'feature_names' in reg_data:\n",
    "            print(\"\\n📋 ویژگی‌های موجود (نمونه):\")\n",
    "            features = reg_data['feature_names']\n",
    "            for i in range(min(10, len(features))):\n",
    "                print(f\"  {i+1}. {features[i]}\")\n",
    "            if len(features) > 10:\n",
    "                print(f\"  ... و {len(features)-10} ویژگی دیگر\")\n",
    "        \n",
    "        # نمونه کد برای استفاده\n",
    "        print(\"\\n💡 نمونه کد برای استفاده:\")\n",
    "        print(\"\"\"```python\n",
    "# بارگذاری داده‌ها\n",
    "X_train = reg_data['X']\n",
    "y_train = reg_data['y']\n",
    "\n",
    "# تقسیم به train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# مدل رگرسیون\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ارزیابی\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f'MSE: {mse:.3f}, R²: {r2:.3f}')\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. داده‌های طبقه‌بندی - تشخیص فیلم‌های محبوب 🎯\n",
    "\n",
    "این بخش برای توسعه‌دهندگانی که روی **طبقه‌بندی دودویی** (فیلم خوب/بد) کار می‌کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'classification' in data['ml_datasets']:\n",
    "    clf_data = data['ml_datasets']['classification']\n",
    "    \n",
    "    if clf_data and 'X' in clf_data:\n",
    "        X_clf = clf_data['X']\n",
    "        y_clf = clf_data['y']\n",
    "        \n",
    "        print(\"🎯 اطلاعات دیتاست طبقه‌بندی:\\n\")\n",
    "        print(f\"تعداد نمونه‌ها: {X_clf.shape[0]:,}\")\n",
    "        print(f\"تعداد ویژگی‌ها: {X_clf.shape[1]}\")\n",
    "        \n",
    "        # توازن کلاس‌ها\n",
    "        if 'class_balance' in clf_data:\n",
    "            balance = clf_data['class_balance']\n",
    "            print(f\"\\nتوازن کلاس‌ها:\")\n",
    "            print(f\"  - فیلم‌های محبوب (امتیاز ≥ 4): {balance.get('positive', balance.get(1, 0)):.1%}\")\n",
    "            print(f\"  - فیلم‌های معمولی (امتیاز < 4): {balance.get('negative', balance.get(0, 0)):.1%}\")\n",
    "        else:\n",
    "            # محاسبه دستی توازن\n",
    "            unique, counts = np.unique(y_clf, return_counts=True)\n",
    "            balance_dict = dict(zip(unique, counts))\n",
    "            total = sum(counts)\n",
    "            \n",
    "        # نمودارها\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # نمودار دایره‌ای توازن کلاس‌ها\n",
    "        if 'class_balance' in clf_data:\n",
    "            labels = ['محبوب (≥4)', 'معمولی (<4)']\n",
    "            sizes = [balance.get('positive', balance.get(1, 0)), \n",
    "                    balance.get('negative', balance.get(0, 0))]\n",
    "            axes[0].pie(sizes, labels=labels, autopct='%1.1f%%', \n",
    "                       colors=[colors[2], colors[3]], startangle=90)\n",
    "        else:\n",
    "            labels = [f'کلاس {c}' for c in unique]\n",
    "            axes[0].pie(counts, labels=labels, autopct='%1.1f%%', \n",
    "                       colors=colors[:len(unique)], startangle=90)\n",
    "        axes[0].set_title('توزیع کلاس‌ها', fontsize=14, pad=20)\n",
    "        \n",
    "        # نمودار میله‌ای\n",
    "        unique_classes, class_counts = np.unique(y_clf, return_counts=True)\n",
    "        axes[1].bar(unique_classes, class_counts, color=[colors[4], colors[5]])\n",
    "        axes[1].set_title('تعداد نمونه‌ها در هر کلاس', fontsize=14, pad=10)\n",
    "        axes[1].set_xlabel('کلاس')\n",
    "        axes[1].set_ylabel('تعداد')\n",
    "        axes[1].set_xticks(unique_classes)\n",
    "        axes[1].set_xticklabels(['معمولی', 'محبوب'])\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # نمونه کد\n",
    "        print(\"\\n💡 نمونه کد برای استفاده:\")\n",
    "        print(\"\"\"```python\n",
    "# بارگذاری داده‌ها\n",
    "X = clf_data['X']\n",
    "y = clf_data['y']\n",
    "\n",
    "# تقسیم داده‌ها\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# مدل طبقه‌بندی\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ارزیابی\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, \n",
    "                          target_names=['معمولی', 'محبوب']))\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. داده‌های خوشه‌بندی - گروه‌بندی کاربران و فیلم‌ها 🎨\n",
    "\n",
    "این بخش برای توسعه‌دهندگانی که روی **خوشه‌بندی** و **تحلیل گروه‌ها** کار می‌کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# خوشه‌بندی کاربران\n",
    "if 'ml_datasets' in data and 'clustering_users' in data['ml_datasets']:\n",
    "    user_cluster_data = data['ml_datasets']['clustering_users']\n",
    "    \n",
    "    if user_cluster_data and 'X' in user_cluster_data:\n",
    "        X_users = user_cluster_data['X']\n",
    "        \n",
    "        print(\"👥 اطلاعات خوشه‌بندی کاربران:\\n\")\n",
    "        print(f\"تعداد کاربران: {X_users.shape[0]:,}\")\n",
    "        print(f\"تعداد ویژگی‌ها: {X_users.shape[1]}\")\n",
    "        \n",
    "        if 'feature_names' in user_cluster_data:\n",
    "            print(\"\\nویژگی‌های کاربران:\")\n",
    "            for i, feat in enumerate(user_cluster_data['feature_names'][:5]):\n",
    "                print(f\"  {i+1}. {feat}\")\n",
    "        \n",
    "        # تجسم با t-SNE\n",
    "        if X_users.shape[0] > 1000:\n",
    "            sample_indices = np.random.choice(X_users.shape[0], 1000, replace=False)\n",
    "            X_sample = X_users[sample_indices]\n",
    "        else:\n",
    "            X_sample = X_users\n",
    "        \n",
    "        # نرمال‌سازی\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_sample)\n",
    "        \n",
    "        # t-SNE\n",
    "        print(\"\\n🔄 در حال محاسبه t-SNE...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        X_tsne = tsne.fit_transform(X_scaled)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                            c=range(len(X_tsne)), cmap='viridis', \n",
    "                            alpha=0.6, s=50)\n",
    "        plt.title('نمایش دوبعدی کاربران با t-SNE', fontsize=16, pad=20)\n",
    "        plt.xlabel('بعد اول')\n",
    "        plt.ylabel('بعد دوم')\n",
    "        plt.colorbar(scatter, label='شاخص کاربر')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# خوشه‌بندی فیلم‌ها\n",
    "if 'ml_datasets' in data and 'clustering_movies' in data['ml_datasets']:\n",
    "    movie_cluster_data = data['ml_datasets']['clustering_movies']\n",
    "    \n",
    "    if movie_cluster_data and 'X' in movie_cluster_data:\n",
    "        X_movies = movie_cluster_data['X']\n",
    "        \n",
    "        print(\"\\n🎬 اطلاعات خوشه‌بندی فیلم‌ها:\\n\")\n",
    "        print(f\"تعداد فیلم‌ها: {X_movies.shape[0]:,}\")\n",
    "        print(f\"تعداد ویژگی‌ها: {X_movies.shape[1]}\")\n",
    "        \n",
    "        # نمونه کد\n",
    "        print(\"\\n💡 نمونه کد برای خوشه‌بندی:\")\n",
    "        print(\"\"\"```python\n",
    "# خوشه‌بندی کاربران\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# نرمال‌سازی داده‌ها\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(user_cluster_data['X'])\n",
    "\n",
    "# تعیین تعداد بهینه خوشه‌ها\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# انتخاب k=5 و خوشه‌بندی نهایی\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "user_clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# تحلیل خوشه‌ها\n",
    "for i in range(5):\n",
    "    cluster_users = np.where(user_clusters == i)[0]\n",
    "    print(f'خوشه {i}: {len(cluster_users)} کاربر')\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. قوانین انجمنی - کشف الگوهای رفتاری 🔍\n",
    "\n",
    "این بخش برای توسعه‌دهندگانی که روی **کشف الگوها** و **قوانین انجمنی** کار می‌کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ml_datasets' in data and 'association_rules' in data['ml_datasets']:\n",
    "    assoc_data = data['ml_datasets']['association_rules']\n",
    "    \n",
    "    if assoc_data and 'transactions' in assoc_data:\n",
    "        transactions = assoc_data['transactions']\n",
    "        \n",
    "        print(\"🔍 اطلاعات قوانین انجمنی:\\n\")\n",
    "        print(f\"تعداد تراکنش‌ها (کاربران): {len(transactions):,}\")\n",
    "        print(f\"حداقل پشتیبانی (min_support): {assoc_data.get('min_support', 'نامشخص')}\")\n",
    "        print(f\"حداقل اطمینان (min_confidence): {assoc_data.get('min_confidence', 'نامشخص')}\")\n",
    "        \n",
    "        # آمار تراکنش‌ها\n",
    "        trans_lengths = [len(t) for t in transactions if t]\n",
    "        if trans_lengths:\n",
    "            print(f\"\\nآمار تراکنش‌ها:\")\n",
    "            print(f\"  - میانگین فیلم‌ها در هر تراکنش: {np.mean(trans_lengths):.1f}\")\n",
    "            print(f\"  - حداقل: {np.min(trans_lengths)}\")\n",
    "            print(f\"  - حداکثر: {np.max(trans_lengths)}\")\n",
    "            \n",
    "            # نمودار توزیع طول تراکنش‌ها\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(trans_lengths, bins=50, color=colors[6], alpha=0.7, edgecolor='black')\n",
    "            plt.title('توزیع تعداد فیلم‌های محبوب هر کاربر', fontsize=14, pad=10)\n",
    "            plt.xlabel('تعداد فیلم‌ها')\n",
    "            plt.ylabel('تعداد کاربران')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        \n",
    "        # نمونه کد\n",
    "        print(\"\\n💡 نمونه کد برای کشف قوانین:\")\n",
    "        print(\"\"\"```python\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# تبدیل تراکنش‌ها به فرمت مناسب\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_transactions = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# کشف آیتم‌های مکرر\n",
    "frequent_items = apriori(df_transactions, \n",
    "                        min_support=0.01, \n",
    "                        use_colnames=True)\n",
    "\n",
    "# استخراج قوانین\n",
    "rules = association_rules(frequent_items, \n",
    "                         metric=\"confidence\", \n",
    "                         min_threshold=0.5)\n",
    "\n",
    "# نمایش قوانین برتر\n",
    "print(\"قوانین با بالاترین اطمینان:\")\n",
    "print(rules.nlargest(10, 'confidence')[['antecedents', 'consequents', \n",
    "                                        'support', 'confidence', 'lift']])\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ماتریس کاربر-آیتم - سیستم توصیه‌گر 🎬\n",
    "\n",
    "این بخش برای توسعه‌دهندگانی که روی **سیستم‌های توصیه‌گر** کار می‌کنند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'user_item_matrix' in data:\n",
    "    matrix = data['user_item_matrix']\n",
    "    \n",
    "    print(\"🎬 اطلاعات ماتریس کاربر-آیتم:\\n\")\n",
    "    print(f\"ابعاد: {matrix.shape[0]:,} کاربر × {matrix.shape[1]:,} فیلم\")\n",
    "    print(f\"تعداد امتیازات ثبت شده: {matrix.nnz:,}\")\n",
    "    print(f\"درصد پراکندگی: {(1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    # توزیع امتیازات\n",
    "    ratings = matrix.data\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # هیستوگرام امتیازات\n",
    "    axes[0].hist(ratings, bins=np.arange(0.5, 6, 0.5), \n",
    "                color=colors[0], alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('توزیع امتیازات در ماتریس', fontsize=14, pad=10)\n",
    "    axes[0].set_xlabel('امتیاز')\n",
    "    axes[0].set_ylabel('تعداد')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # آمار کاربران\n",
    "    user_counts = np.array((matrix != 0).sum(axis=1)).flatten()\n",
    "    axes[1].hist(user_counts[user_counts > 0], bins=50, \n",
    "                color=colors[1], alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('توزیع تعداد امتیازات هر کاربر', fontsize=14, pad=10)\n",
    "    axes[1].set_xlabel('تعداد امتیاز')\n",
    "    axes[1].set_ylabel('تعداد کاربران')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # نمونه کد\n",
    "    print(\"\\n💡 نمونه کد برای سیستم توصیه‌گر:\")\n",
    "    print(\"\"\"```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# محاسبه شباهت بین کاربران\n",
    "user_similarity = cosine_similarity(matrix, dense_output=False)\n",
    "\n",
    "# پیش‌بینی امتیاز برای کاربر i و فیلم j\n",
    "def predict_rating(user_id, movie_id, k=50):\n",
    "    # یافتن k کاربر مشابه\n",
    "    user_sim = user_similarity[user_id].toarray().flatten()\n",
    "    similar_users = np.argsort(user_sim)[-k-1:-1][::-1]\n",
    "    \n",
    "    # محاسبه میانگین وزن‌دار\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    \n",
    "    for similar_user in similar_users:\n",
    "        if matrix[similar_user, movie_id] > 0:\n",
    "            numerator += user_sim[similar_user] * matrix[similar_user, movie_id]\n",
    "            denominator += user_sim[similar_user]\n",
    "    \n",
    "    if denominator > 0:\n",
    "        return numerator / denominator\n",
    "    else:\n",
    "        return matrix[user_id].mean()  # میانگین امتیازات کاربر\n",
    "\n",
    "# توصیه فیلم‌ها برای یک کاربر\n",
    "def recommend_movies(user_id, n_recommendations=10):\n",
    "    # فیلم‌هایی که کاربر ندیده\n",
    "    user_ratings = matrix[user_id].toarray().flatten()\n",
    "    unseen_movies = np.where(user_ratings == 0)[0]\n",
    "    \n",
    "    # پیش‌بینی امتیاز برای فیلم‌های ندیده\n",
    "    predictions = []\n",
    "    for movie_id in unseen_movies[:1000]:  # محدود به 1000 فیلم برای سرعت\n",
    "        pred = predict_rating(user_id, movie_id)\n",
    "        predictions.append((movie_id, pred))\n",
    "    \n",
    "    # مرتب‌سازی و انتخاب بهترین‌ها\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:n_recommendations]\n",
    "```\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ویژگی‌های کاربران و فیلم‌ها 📊\n",
    "\n",
    "بررسی ویژگی‌های استخراج شده برای کاربران و فیلم‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ویژگی‌های کاربران\n",
    "if 'user_features' in data:\n",
    "    user_features = data['user_features']\n",
    "    print(\"👥 ویژگی‌های کاربران:\\n\")\n",
    "    print(f\"تعداد کاربران: {len(user_features):,}\")\n",
    "    print(f\"تعداد ویژگی‌ها: {len(user_features.columns)}\")\n",
    "    print(\"\\nنمونه ویژگی‌ها:\")\n",
    "    for col in user_features.columns[:10]:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # نمایش آمار توصیفی\n",
    "    print(\"\\nآمار توصیفی (5 ویژگی اول):\")\n",
    "    print(user_features.iloc[:, :5].describe().round(2))\n",
    "\n",
    "# ویژگی‌های فیلم‌ها\n",
    "if 'movie_features' in data:\n",
    "    movie_features = data['movie_features']\n",
    "    print(\"\\n\\n🎬 ویژگی‌های فیلم‌ها:\\n\")\n",
    "    print(f\"تعداد فیلم‌ها: {len(movie_features):,}\")\n",
    "    print(f\"تعداد ویژگی‌ها: {len(movie_features.columns)}\")\n",
    "    print(\"\\nنمونه ویژگی‌ها:\")\n",
    "    for col in movie_features.columns[:10]:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. خلاصه و راهنمای استفاده 📚\n",
    "\n",
    "### خلاصه داده‌های موجود:\n",
    "\n",
    "1. **رگرسیون**: پیش‌بینی امتیاز دقیق فیلم‌ها (0.5 تا 5)\n",
    "2. **طبقه‌بندی**: تشخیص فیلم‌های محبوب (امتیاز ≥ 4) از معمولی\n",
    "3. **خوشه‌بندی کاربران**: گروه‌بندی کاربران بر اساس رفتار\n",
    "4. **خوشه‌بندی فیلم‌ها**: گروه‌بندی فیلم‌ها بر اساس ویژگی‌ها\n",
    "5. **قوانین انجمنی**: کشف الگوهای مشترک در انتخاب فیلم‌ها\n",
    "6. **ماتریس کاربر-آیتم**: برای سیستم‌های توصیه‌گر مبتنی بر فیلترینگ مشارکتی\n",
    "\n",
    "### نکات مهم:\n",
    "\n",
    "- همه داده‌ها از قبل پردازش و آماده شده‌اند\n",
    "- ویژگی‌ها نرمال‌سازی شده و آماده استفاده در مدل‌ها هستند\n",
    "- برای جلوگیری از overfitting، از تقسیم train/test استفاده کنید\n",
    "- در صورت نیاز به ویژگی‌های اضافی، با تیم پیش‌پردازش هماهنگ کنید"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ذخیره‌سازی و صدور داده‌ها 💾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تابع کمکی برای ذخیره داده‌ها\n",
    "def save_dataset_for_task(task_name, output_format='csv'):\n",
    "    \"\"\"ذخیره دیتاست یک تسک خاص\"\"\"\n",
    "    \n",
    "    if 'ml_datasets' not in data or task_name not in data['ml_datasets']:\n",
    "        print(f\"❌ دیتاست {task_name} یافت نشد!\")\n",
    "        return\n",
    "    \n",
    "    task_data = data['ml_datasets'][task_name]\n",
    "    if not task_data:\n",
    "        print(f\"❌ دیتاست {task_name} خالی است!\")\n",
    "        return\n",
    "    \n",
    "    # ایجاد دایرکتوری خروجی\n",
    "    import os\n",
    "    os.makedirs('exported_data', exist_ok=True)\n",
    "    \n",
    "    if 'X' in task_data and 'y' in task_data:\n",
    "        # تبدیل به DataFrame\n",
    "        X_df = pd.DataFrame(task_data['X'])\n",
    "        if 'feature_names' in task_data:\n",
    "            X_df.columns = task_data['feature_names']\n",
    "        \n",
    "        y_df = pd.DataFrame(task_data['y'], columns=['target'])\n",
    "        \n",
    "        # ترکیب X و y\n",
    "        full_df = pd.concat([X_df, y_df], axis=1)\n",
    "        \n",
    "        # ذخیره\n",
    "        if output_format == 'csv':\n",
    "            full_df.to_csv(f'exported_data/{task_name}_dataset.csv', index=False)\n",
    "            print(f\"✅ دیتاست {task_name} در فایل exported_data/{task_name}_dataset.csv ذخیره شد\")\n",
    "        elif output_format == 'parquet':\n",
    "            full_df.to_parquet(f'exported_data/{task_name}_dataset.parquet', index=False)\n",
    "            print(f\"✅ دیتاست {task_name} در فایل exported_data/{task_name}_dataset.parquet ذخیره شد\")\n",
    "\n",
    "# مثال استفاده\n",
    "print(\"📥 برای ذخیره دیتاست‌ها از دستورات زیر استفاده کنید:\\n\")\n",
    "print(\"save_dataset_for_task('regression', 'csv')    # ذخیره دیتاست رگرسیون\")\n",
    "print(\"save_dataset_for_task('classification', 'parquet')  # ذخیره دیتاست طبقه‌بندی\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🎯 موفق باشید!\n",
    "\n",
    "این نوت‌بوک توسط تیم پیش‌پردازش داده آماده شده است. در صورت نیاز به اطلاعات بیشتر یا ویژگی‌های اضافی، با ما در ارتباط باشید.\n",
    "\n",
    "**نسخه**: 1.0  \n",
    "**تاریخ**: ۱۴۰۳/۰۹/۲۰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movielens-viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
